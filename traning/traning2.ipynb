{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dbca2a-c92f-41be-81ba-ff11648a7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class PlantDiseaseClassifier:\n",
    "    def __init__(self, data_dir, img_size=(224, 224), batch_size=32):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.class_names = None\n",
    "        self.plant_names = None\n",
    "        self.disease_names = None\n",
    "        self.class_mapping = {}  # Maps class_index to (plant, disease)\n",
    "        \n",
    "        # Medicine recommendations\n",
    "        self.medicine_dict = {\n",
    "            \"apple_healthy\": \"No treatment needed - maintain good plant hygiene\",\n",
    "            \"apple_scab\": \"Fungicide: Captan, Mancozeb, or Sulfur spray. Remove fallen leaves.\",\n",
    "            \"apple_powdery_mildew\": \"Fungicide: Sulfur spray, Potassium bicarbonate, or Neem oil\",\n",
    "            \"apple_rust\": \"Fungicide: Myclobutanil or Tebuconazole. Remove infected leaves.\",\n",
    "            \"apple_black_rot\": \"Fungicide: Copper-based spray or Captan. Prune infected areas.\",\n",
    "            \"tomato_healthy\": \"No treatment needed - maintain good plant hygiene\",\n",
    "            \"tomato_early_blight\": \"Fungicide: Chlorothalonil or Copper-based spray. Improve air circulation.\",\n",
    "            \"tomato_late_blight\": \"Fungicide: Fluazinam or Metalaxyl. Remove infected plants immediately.\",\n",
    "            \"tomato_bacterial_spot\": \"Copper-based bactericide. Avoid overhead watering.\",\n",
    "            \"tomato_leaf_mold\": \"Improve ventilation. Fungicide: Chlorothalonil if severe.\",\n",
    "            \"tomato_septoria_leaf_spot\": \"Fungicide: Copper-based spray. Remove lower leaves.\",\n",
    "            \"tomato_spider_mites\": \"Miticide: Abamectin or introduce predatory mites. Increase humidity.\",\n",
    "            \"tomato_target_spot\": \"Fungicide: Azoxystrobin or Chlorothalonil. Remove debris.\",\n",
    "            \"tomato_mosaic_virus\": \"No cure - remove infected plants. Control aphids to prevent spread.\",\n",
    "            \"tomato_yellow_leaf_curl\": \"Remove infected plants. Control whiteflies. Use resistant varieties.\"\n",
    "        }\n",
    "\n",
    "    def scan_dataset_structure(self):\n",
    "        \"\"\"Scan the dataset to understand the structure and create class mappings\"\"\"\n",
    "        print(\"Scanning dataset structure...\")\n",
    "        \n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise FileNotFoundError(f\"Dataset directory '{self.data_dir}' not found.\")\n",
    "        \n",
    "        plants = []\n",
    "        diseases = []\n",
    "        class_names = []\n",
    "        class_mapping = {}\n",
    "        \n",
    "        class_index = 0\n",
    "        for plant_name in sorted(os.listdir(self.data_dir)):\n",
    "            plant_path = os.path.join(self.data_dir, plant_name)\n",
    "            if os.path.isdir(plant_path):\n",
    "                print(f\"\\nFound plant: {plant_name}\")\n",
    "                for disease_name in sorted(os.listdir(plant_path)):\n",
    "                    disease_path = os.path.join(plant_path, disease_name)\n",
    "                    if os.path.isdir(disease_path):\n",
    "                        # Count images in this category\n",
    "                        image_count = len([f for f in os.listdir(disease_path) \n",
    "                                         if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))])\n",
    "                        print(f\"  - {disease_name}: {image_count} images\")\n",
    "                        \n",
    "                        # Create class name and mapping\n",
    "                        class_name = f\"{plant_name}_{disease_name}\"\n",
    "                        class_names.append(class_name)\n",
    "                        class_mapping[class_index] = (plant_name, disease_name)\n",
    "                        \n",
    "                        if plant_name not in plants:\n",
    "                            plants.append(plant_name)\n",
    "                        if disease_name not in diseases:\n",
    "                            diseases.append(disease_name)\n",
    "                        \n",
    "                        class_index += 1\n",
    "        \n",
    "        self.plant_names = plants\n",
    "        self.disease_names = diseases\n",
    "        self.class_names = class_names\n",
    "        self.class_mapping = class_mapping\n",
    "        \n",
    "        print(f\"\\nDataset Summary:\")\n",
    "        print(f\"Total plants: {len(plants)} - {plants}\")\n",
    "        print(f\"Total diseases: {len(diseases)} - {diseases}\")\n",
    "        print(f\"Total classes: {len(class_names)}\")\n",
    "        \n",
    "        return class_names, class_mapping\n",
    "\n",
    "    def create_flat_structure(self):\n",
    "        \"\"\"Create a flat structure for keras to process\"\"\"\n",
    "        flat_dir = \"temp_flat_dataset\"\n",
    "        if os.path.exists(flat_dir):\n",
    "            shutil.rmtree(flat_dir)\n",
    "        os.makedirs(flat_dir)\n",
    "        \n",
    "        print(\"Creating flat dataset structure...\")\n",
    "        \n",
    "        for plant_name in os.listdir(self.data_dir):\n",
    "            plant_path = os.path.join(self.data_dir, plant_name)\n",
    "            if os.path.isdir(plant_path):\n",
    "                for disease_name in os.listdir(plant_path):\n",
    "                    disease_path = os.path.join(plant_path, disease_name)\n",
    "                    if os.path.isdir(disease_path):\n",
    "                        # Create flattened class name\n",
    "                        flat_class_name = f\"{plant_name}_{disease_name}\"\n",
    "                        flat_class_path = os.path.join(flat_dir, flat_class_name)\n",
    "                        \n",
    "                        # Copy the directory\n",
    "                        shutil.copytree(disease_path, flat_class_path)\n",
    "        \n",
    "        return flat_dir\n",
    "\n",
    "    def load_data(self, validation_split=0.2):\n",
    "        \"\"\"Load and prepare the dataset\"\"\"\n",
    "        # First scan the structure\n",
    "        self.scan_dataset_structure()\n",
    "        \n",
    "        # Create flat structure\n",
    "        flat_data_dir = self.create_flat_structure()\n",
    "        \n",
    "        print(f\"\\nLoading dataset with {len(self.class_names)} classes...\")\n",
    "        \n",
    "        # Create datasets\n",
    "        self.train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            flat_data_dir,\n",
    "            validation_split=validation_split,\n",
    "            subset=\"training\",\n",
    "            seed=42,\n",
    "            image_size=self.img_size,\n",
    "            batch_size=self.batch_size,\n",
    "            class_names=self.class_names  # Ensure consistent class ordering\n",
    "        )\n",
    "        \n",
    "        self.val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            flat_data_dir,\n",
    "            validation_split=validation_split,\n",
    "            subset=\"validation\",\n",
    "            seed=42,\n",
    "            image_size=self.img_size,\n",
    "            batch_size=self.batch_size,\n",
    "            class_names=self.class_names  # Ensure consistent class ordering\n",
    "        )\n",
    "        \n",
    "        print(f\"Training samples: {len(self.train_ds) * self.batch_size}\")\n",
    "        print(f\"Validation samples: {len(self.val_ds) * self.batch_size}\")\n",
    "        \n",
    "        # Optimize dataset performance\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_ds = self.train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "        self.val_ds = self.val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        \n",
    "        return self.train_ds, self.val_ds\n",
    "\n",
    "    def create_data_augmentation(self):\n",
    "        \"\"\"Create data augmentation layers\"\"\"\n",
    "        return keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "            layers.RandomRotation(0.2),\n",
    "            layers.RandomZoom(0.2),\n",
    "            layers.RandomContrast(0.2),\n",
    "            layers.RandomBrightness(0.2),\n",
    "        ])\n",
    "\n",
    "    def create_model(self, use_transfer_learning=True):\n",
    "        \"\"\"Create the model architecture\"\"\"\n",
    "        num_classes = len(self.class_names)\n",
    "        \n",
    "        if use_transfer_learning:\n",
    "            # Transfer learning with MobileNetV2\n",
    "            base_model = tf.keras.applications.MobileNetV2(\n",
    "                input_shape=(*self.img_size, 3),\n",
    "                include_top=False,\n",
    "                weights='imagenet'\n",
    "            )\n",
    "            base_model.trainable = False\n",
    "            \n",
    "            data_augmentation = self.create_data_augmentation()\n",
    "            preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "            \n",
    "            inputs = keras.Input(shape=(*self.img_size, 3))\n",
    "            x = data_augmentation(inputs)\n",
    "            x = preprocess_input(x)\n",
    "            x = base_model(x, training=False)\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "            outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "            model = keras.Model(inputs, outputs)\n",
    "        else:\n",
    "            # Custom CNN architecture\n",
    "            data_augmentation = self.create_data_augmentation()\n",
    "            model = keras.Sequential([\n",
    "                data_augmentation,\n",
    "                layers.Rescaling(1./255),\n",
    "                layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling2D(),\n",
    "                layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling2D(),\n",
    "                layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling2D(),\n",
    "                layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling2D(),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(512, activation='relu'),\n",
    "                layers.Dropout(0.5),\n",
    "                layers.Dense(256, activation='relu'),\n",
    "                layers.Dropout(0.3),\n",
    "                layers.Dense(num_classes, activation='softmax')\n",
    "            ])\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def compile_model(self, learning_rate=0.001):\n",
    "        \"\"\"Compile the model\"\"\"\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(\"Model compiled successfully!\")\n",
    "        print(f\"Model has {self.model.count_params():,} parameters\")\n",
    "\n",
    "    def train_model(self, epochs=20, callbacks=None):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if callbacks is None:\n",
    "            callbacks = [\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss', \n",
    "                    patience=5, \n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                ),\n",
    "                keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss', \n",
    "                    factor=0.2, \n",
    "                    patience=3, \n",
    "                    min_lr=1e-7,\n",
    "                    verbose=1\n",
    "                )\n",
    "            ]\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        self.history = self.model.fit(\n",
    "            self.train_ds,\n",
    "            validation_data=self.val_ds,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        return self.history\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"No training history available. Train the model first.\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        ax1.plot(self.history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "        ax1.plot(self.history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "        ax1.set_title('Model Accuracy')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot loss\n",
    "        ax2.plot(self.history.history['loss'], label='Training Loss', marker='o')\n",
    "        ax2.plot(self.history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "        ax2.set_title('Model Loss')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Evaluate the model and show detailed metrics\"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        \n",
    "        y_pred, y_true = [], []\n",
    "        for images, labels in self.val_ds:\n",
    "            predictions = self.model.predict(images, verbose=0)\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "            y_true.extend(labels.numpy())\n",
    "        \n",
    "        # Get unique classes present in the validation set\n",
    "        unique_classes = sorted(list(set(y_true + y_pred)))\n",
    "        present_class_names = [self.class_names[i] for i in unique_classes]\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred, \n",
    "                                  labels=unique_classes,\n",
    "                                  target_names=present_class_names))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=unique_classes)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=present_class_names, \n",
    "                    yticklabels=present_class_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        accuracy = np.sum(np.array(y_true) == np.array(y_pred)) / len(y_true)\n",
    "        print(f\"\\nOverall Validation Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Show per-plant and per-disease accuracy\n",
    "        self.show_detailed_results(y_true, y_pred, unique_classes)\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "    def show_detailed_results(self, y_true, y_pred, unique_classes):\n",
    "        \"\"\"Show detailed results by plant and disease\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DETAILED RESULTS BY PLANT AND DISEASE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Group results by plant\n",
    "        plant_results = {}\n",
    "        disease_results = {}\n",
    "        \n",
    "        for i, (true_idx, pred_idx) in enumerate(zip(y_true, y_pred)):\n",
    "            if true_idx in unique_classes and pred_idx in unique_classes:\n",
    "                true_plant, true_disease = self.class_mapping[true_idx]\n",
    "                pred_plant, pred_disease = self.class_mapping[pred_idx]\n",
    "                \n",
    "                # Plant-level accuracy\n",
    "                if true_plant not in plant_results:\n",
    "                    plant_results[true_plant] = {'correct': 0, 'total': 0}\n",
    "                plant_results[true_plant]['total'] += 1\n",
    "                if true_plant == pred_plant:\n",
    "                    plant_results[true_plant]['correct'] += 1\n",
    "                \n",
    "                # Disease-level accuracy\n",
    "                if true_disease not in disease_results:\n",
    "                    disease_results[true_disease] = {'correct': 0, 'total': 0}\n",
    "                disease_results[true_disease]['total'] += 1\n",
    "                if true_disease == pred_disease:\n",
    "                    disease_results[true_disease]['correct'] += 1\n",
    "        \n",
    "        # Display plant-level results\n",
    "        print(\"\\nPLANT IDENTIFICATION ACCURACY:\")\n",
    "        print(\"-\" * 40)\n",
    "        for plant, results in plant_results.items():\n",
    "            accuracy = results['correct'] / results['total']\n",
    "            print(f\"{plant:15}: {accuracy:.3f} ({results['correct']}/{results['total']})\")\n",
    "        \n",
    "        # Display disease-level results\n",
    "        print(\"\\nDISEASE IDENTIFICATION ACCURACY:\")\n",
    "        print(\"-\" * 40)\n",
    "        for disease, results in disease_results.items():\n",
    "            accuracy = results['correct'] / results['total']\n",
    "            print(f\"{disease:20}: {accuracy:.3f} ({results['correct']}/{results['total']})\")\n",
    "\n",
    "    def predict_image(self, image_path, show_image=True):\n",
    "        \"\"\"Predict disease for a single image\"\"\"\n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFoundError(f\"Image file '{image_path}' not found.\")\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img = keras.utils.load_img(image_path, target_size=self.img_size)\n",
    "        img_array = keras.utils.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array, 0)\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = self.model.predict(img_array, verbose=0)\n",
    "        predicted_class_idx = np.argmax(predictions[0])\n",
    "        confidence = float(np.max(predictions[0]))\n",
    "        \n",
    "        # Get plant and disease names\n",
    "        plant_name, disease_name = self.class_mapping[predicted_class_idx]\n",
    "        class_name = self.class_names[predicted_class_idx]\n",
    "        \n",
    "        # Get medicine recommendation\n",
    "        medicine = self.medicine_dict.get(class_name, \"No specific treatment recommendation available\")\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PLANT DISEASE DIAGNOSIS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Plant Type:     {plant_name.upper()}\")\n",
    "        print(f\"Disease:        {disease_name.upper()}\")\n",
    "        print(f\"Confidence:     {confidence:.2%}\")\n",
    "        print(f\"Treatment:      {medicine}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Show image if requested\n",
    "        if show_image:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Plant: {plant_name} | Disease: {disease_name}\\nConfidence: {confidence:.2%}\")\n",
    "            plt.show()\n",
    "        \n",
    "        return plant_name, disease_name, confidence, medicine\n",
    "\n",
    "    def save_model(self, filename=\"plant_disease_model.keras\"):\n",
    "        \"\"\"Save the trained model and metadata to ../models directory\"\"\"\n",
    "        # Create the models directory\n",
    "        models_dir = \"../models\"\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "        \n",
    "        # Full filepath\n",
    "        filepath = os.path.join(models_dir, filename)\n",
    "        \n",
    "        # Save the model\n",
    "        self.model.save(filepath)\n",
    "        \n",
    "        # Save class mappings and metadata\n",
    "        import pickle\n",
    "        metadata = {\n",
    "            'class_names': self.class_names,\n",
    "            'class_mapping': self.class_mapping,\n",
    "            'plant_names': self.plant_names,\n",
    "            'disease_names': self.disease_names,\n",
    "            'img_size': self.img_size,\n",
    "            'medicine_dict': self.medicine_dict\n",
    "        }\n",
    "        \n",
    "        metadata_filename = filename.replace('.keras', '_metadata.pkl')\n",
    "        metadata_filepath = os.path.join(models_dir, metadata_filename)\n",
    "        \n",
    "        with open(metadata_filepath, 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"Model saved to {filepath}\")\n",
    "        print(f\"Metadata saved to {metadata_filepath}\")\n",
    "        \n",
    "        # List all files in the models directory\n",
    "        print(f\"\\nFiles in {models_dir}:\")\n",
    "        files = os.listdir(models_dir)\n",
    "        for file in sorted(files):\n",
    "            file_path = os.path.join(models_dir, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "                print(f\"  - {file} ({size:.2f} MB)\")\n",
    "\n",
    "    def load_model(self, filename=\"plant_disease_model.keras\"):\n",
    "        \"\"\"Load a saved model and metadata from ../models directory\"\"\"\n",
    "        models_dir = \"../models\"\n",
    "        filepath = os.path.join(models_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Model file '{filepath}' not found.\")\n",
    "        \n",
    "        # Load the model\n",
    "        self.model = keras.models.load_model(filepath)\n",
    "        \n",
    "        # Load class mappings and metadata\n",
    "        import pickle\n",
    "        metadata_filename = filename.replace('.keras', '_metadata.pkl')\n",
    "        metadata_filepath = os.path.join(models_dir, metadata_filename)\n",
    "        \n",
    "        if os.path.exists(metadata_filepath):\n",
    "            with open(metadata_filepath, 'rb') as f:\n",
    "                metadata = pickle.load(f)\n",
    "            \n",
    "            self.class_names = metadata['class_names']\n",
    "            self.class_mapping = metadata['class_mapping']\n",
    "            self.plant_names = metadata['plant_names']\n",
    "            self.disease_names = metadata['disease_names']\n",
    "            self.img_size = metadata['img_size']\n",
    "            \n",
    "            # Load medicine dictionary if available (for backward compatibility)\n",
    "            if 'medicine_dict' in metadata:\n",
    "                self.medicine_dict = metadata['medicine_dict']\n",
    "            \n",
    "            print(f\"Model and metadata loaded from {filepath}\")\n",
    "            print(f\"Model supports {len(self.class_names)} classes:\")\n",
    "            for i, class_name in enumerate(self.class_names):\n",
    "                plant, disease = self.class_mapping[i]\n",
    "                print(f\"  {i}: {plant} - {disease}\")\n",
    "        else:\n",
    "            print(f\"Model loaded from {filepath}, but metadata file not found\")\n",
    "            print(\"You may need to retrain or manually set class mappings\")\n",
    "\n",
    "    def list_saved_models(self):\n",
    "        \"\"\"List all saved models in the ../models directory\"\"\"\n",
    "        models_dir = \"../models\"\n",
    "        if not os.path.exists(models_dir):\n",
    "            print(f\"Models directory '{models_dir}' does not exist.\")\n",
    "            return []\n",
    "        \n",
    "        model_files = []\n",
    "        files = os.listdir(models_dir)\n",
    "        \n",
    "        print(f\"\\nSaved models in {models_dir}:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for file in sorted(files):\n",
    "            if file.endswith('.keras'):\n",
    "                file_path = os.path.join(models_dir, file)\n",
    "                size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "                \n",
    "                # Check for corresponding metadata file\n",
    "                metadata_file = file.replace('.keras', '_metadata.pkl')\n",
    "                metadata_path = os.path.join(models_dir, metadata_file)\n",
    "                has_metadata = os.path.exists(metadata_path)\n",
    "                \n",
    "                model_files.append(file)\n",
    "                status = \"✓\" if has_metadata else \"✗\"\n",
    "                print(f\"  {status} {file} ({size:.2f} MB)\")\n",
    "                if has_metadata:\n",
    "                    metadata_size = os.path.getsize(metadata_path) / 1024  # Size in KB\n",
    "                    print(f\"    └── {metadata_file} ({metadata_size:.2f} KB)\")\n",
    "        \n",
    "        if not model_files:\n",
    "            print(\"  No saved models found.\")\n",
    "        \n",
    "        return model_files\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up temporary files\"\"\"\n",
    "        if os.path.exists(\"temp_flat_dataset\"):\n",
    "            shutil.rmtree(\"temp_flat_dataset\")\n",
    "            print(\"Temporary files cleaned up.\")\n",
    "\n",
    "def create_sample_dataset():\n",
    "    \"\"\"Create a sample dataset structure for testing\"\"\"\n",
    "    print(\"Creating sample dataset structure...\")\n",
    "    dataset_dir = \"my_diseases_dataset\"\n",
    "    \n",
    "    # Create nested directory structure - Only Apple and Tomato\n",
    "    plant_diseases = {\n",
    "        \"apple\": [\"healthy\", \"scab\", \"rust\", \"powdery_mildew\"],\n",
    "        \"tomato\": [\"healthy\", \"early_blight\", \"late_blight\", \"bacterial_spot\"]\n",
    "    }\n",
    "    \n",
    "    for plant, diseases in plant_diseases.items():\n",
    "        for disease in diseases:\n",
    "            path = os.path.join(dataset_dir, plant, disease)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Sample dataset structure created at: {dataset_dir}\")\n",
    "    print(\"\\nDirectory structure:\")\n",
    "    for plant, diseases in plant_diseases.items():\n",
    "        print(f\"{plant}/\")\n",
    "        for disease in diseases:\n",
    "            print(f\"  ├── {disease}/\")\n",
    "    \n",
    "    print(\"\\nPlease add your plant disease images to the respective folders.\")\n",
    "    return dataset_dir\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate usage\"\"\"\n",
    "    data_dir = \"my_diseases_dataset\"\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Dataset directory '{data_dir}' not found.\")\n",
    "        print(\"\\nOptions:\")\n",
    "        print(\"1. Create sample structure with create_sample_dataset()\")\n",
    "        print(\"2. Or put your existing dataset in the 'my_diseases_dataset' folder\")\n",
    "        print(\"3. Or change 'data_dir' variable to point to your dataset location\")\n",
    "        create_sample_dataset()\n",
    "        print(\"Please add your images to the dataset folders and run again.\")\n",
    "        return\n",
    "    \n",
    "    # First, let's see what's in your dataset\n",
    "    print(\"Scanning your dataset...\")\n",
    "    dataset_structure = scan_existing_dataset(data_dir)\n",
    "    \n",
    "    if dataset_structure is None:\n",
    "        return\n",
    "    \n",
    "    # Ask user if they want to continue\n",
    "    print(f\"\\nDo you want to train a model with this dataset structure? (y/n)\")\n",
    "    # For automatic execution, we'll continue. In interactive mode, you'd wait for input.\n",
    "    \n",
    "    # Initialize classifier\n",
    "    classifier = PlantDiseaseClassifier(data_dir, img_size=(224, 224), batch_size=32)\n",
    "    \n",
    "    try:\n",
    "        # Load and scan data\n",
    "        train_ds, val_ds = classifier.load_data(validation_split=0.2)\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = classifier.create_model(use_transfer_learning=True)\n",
    "        classifier.compile_model(learning_rate=0.001)\n",
    "        \n",
    "        # Train model\n",
    "        history = classifier.train_model(epochs=20)\n",
    "        \n",
    "        # Plot training history\n",
    "        classifier.plot_training_history()\n",
    "        \n",
    "        # Evaluate model\n",
    "        accuracy = classifier.evaluate_model()\n",
    "        \n",
    "        # Save model to ../models directory\n",
    "        classifier.save_model(\"plant_disease_model.keras\")\n",
    "        \n",
    "        # List all saved models\n",
    "        classifier.list_saved_models()\n",
    "        \n",
    "        print(f\"\\nTraining completed successfully!\")\n",
    "        print(f\"Final validation accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Example of loading the model\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Testing model loading...\")\n",
    "        new_classifier = PlantDiseaseClassifier(data_dir)\n",
    "        new_classifier.load_model(\"plant_disease_model.keras\")\n",
    "        \n",
    "        # Example prediction (uncomment and provide image path)\n",
    "        # classifier.predict_image(\"path/to/your/test_image.jpg\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        classifier.cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
