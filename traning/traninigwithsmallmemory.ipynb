{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27169f29-4f26-4975-8a82-dff2c3737284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing classifier with memory optimization...\n",
      "Loading dataset...\n",
      "Scanning dataset structure...\n",
      "\n",
      "Found plant: apple\n",
      "  - Alternaria leaf spot: 278 images\n",
      "  - Brown spot: 215 images\n",
      "  - Gray spot: 395 images\n",
      "  - Healthy leaf: 409 images\n",
      "  - Rust: 344 images\n",
      "\n",
      "Found plant: tomato\n",
      "  - Tomato___Bacterial_spot: 1000 images\n",
      "  - Tomato___Early_blight: 1000 images\n",
      "  - Tomato___Late_blight: 1000 images\n",
      "  - Tomato___Leaf_Mold: 1000 images\n",
      "  - Tomato___Septoria_leaf_spot: 1000 images\n",
      "  - Tomato___Spider_mites Two-spotted_spider_mite: 1000 images\n",
      "  - Tomato___Target_Spot: 9 images\n",
      "\n",
      "Dataset Summary:\n",
      "Total plants: 2 - ['apple', 'tomato']\n",
      "Total diseases: 12 - ['Alternaria leaf spot', 'Brown spot', 'Gray spot', 'Healthy leaf', 'Rust', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot']\n",
      "Total classes: 12\n",
      "Cleaning up old temporary directory: temp_flat_dataset_5c9364a0\n",
      "Successfully removed temp_flat_dataset_5c9364a0\n",
      "Creating flat dataset structure in temp_flat_dataset_e8188234...\n",
      "Flat structure created successfully in temp_flat_dataset_e8188234\n",
      "\n",
      "Loading dataset with 12 classes...\n",
      "Using batch size: 8\n",
      "Found 7650 files belonging to 12 classes.\n",
      "Using 6120 files for training.\n",
      "Found 7650 files belonging to 12 classes.\n",
      "Using 1530 files for validation.\n",
      "Training samples: 6120\n",
      "Validation samples: 1536\n",
      "Creating model...\n",
      "Using lightweight MobileNetV2 (alpha=0.75)\n",
      "Model compiled successfully!\n",
      "Model has 1,397,436 parameters\n",
      "Training model...\n",
      "Starting training...\n",
      "Batch size: 8\n",
      "Max epochs: 10\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import time\n",
    "import stat\n",
    "import gc\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Memory optimization settings\n",
    "tf.config.experimental.enable_memory_growth = True\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduce TensorFlow logging\n",
    "\n",
    "class PlantDiseaseClassifier:\n",
    "    def __init__(self, data_dir, img_size=(224, 224), batch_size=16):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.class_names = None\n",
    "        self.plant_names = None\n",
    "        self.disease_names = None\n",
    "        self.class_mapping = {}\n",
    "        self.temp_dir = None\n",
    "        self.train_ds = None\n",
    "        self.val_ds = None\n",
    "\n",
    "    def robust_rmtree(self, path, max_retries=5, delay=1):\n",
    "        \"\"\"Robust directory removal for Windows\"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            return True\n",
    "            \n",
    "        # First, try to close any TensorFlow datasets that might be using the files\n",
    "        if hasattr(self, 'train_ds') and self.train_ds is not None:\n",
    "            del self.train_ds\n",
    "        if hasattr(self, 'val_ds') and self.val_ds is not None:\n",
    "            del self.val_ds\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Change permissions recursively to make files deletable\n",
    "                for root, dirs, files in os.walk(path):\n",
    "                    for d in dirs:\n",
    "                        os.chmod(os.path.join(root, d), stat.S_IWRITE)\n",
    "                    for f in files:\n",
    "                        file_path = os.path.join(root, f)\n",
    "                        try:\n",
    "                            os.chmod(file_path, stat.S_IWRITE)\n",
    "                        except:\n",
    "                            pass\n",
    "                \n",
    "                # Try to remove the directory\n",
    "                shutil.rmtree(path, ignore_errors=False)\n",
    "                print(f\"Successfully removed {path}\")\n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed to remove {path}: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Waiting {delay} seconds before retry...\")\n",
    "                    time.sleep(delay)\n",
    "                    delay *= 2  # Exponential backoff\n",
    "                else:\n",
    "                    print(f\"Failed to remove {path} after {max_retries} attempts\")\n",
    "                    # Try alternative approach\n",
    "                    return self.alternative_cleanup(path)\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def alternative_cleanup(self, path):\n",
    "        \"\"\"Alternative cleanup method - rename and schedule for deletion\"\"\"\n",
    "        try:\n",
    "            # Rename to a temp name for later cleanup\n",
    "            import uuid\n",
    "            temp_name = f\"temp_to_delete_{uuid.uuid4().hex[:8]}\"\n",
    "            new_path = os.path.join(os.path.dirname(path), temp_name)\n",
    "            os.rename(path, new_path)\n",
    "            print(f\"Renamed {path} to {new_path} for later cleanup\")\n",
    "            \n",
    "            # Try to remove files individually\n",
    "            self.remove_files_individually(new_path)\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Alternative cleanup also failed: {e}\")\n",
    "            print(\"You may need to manually delete the temporary directory later\")\n",
    "            return False\n",
    "\n",
    "    def remove_files_individually(self, path):\n",
    "        \"\"\"Remove files one by one\"\"\"\n",
    "        for root, dirs, files in os.walk(path, topdown=False):\n",
    "            # Remove files first\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    os.chmod(file_path, stat.S_IWRITE)\n",
    "                    os.remove(file_path)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Then remove empty directories\n",
    "            for dir in dirs:\n",
    "                dir_path = os.path.join(root, dir)\n",
    "                try:\n",
    "                    os.rmdir(dir_path)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Finally try to remove the root directory\n",
    "        try:\n",
    "            os.rmdir(path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def scan_dataset_structure(self):\n",
    "        \"\"\"Scan the dataset to understand the structure and create class mappings\"\"\"\n",
    "        print(\"Scanning dataset structure...\")\n",
    "        \n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise FileNotFoundError(f\"Dataset directory '{self.data_dir}' not found.\")\n",
    "        \n",
    "        plants = []\n",
    "        diseases = []\n",
    "        class_names = []\n",
    "        class_mapping = {}\n",
    "        \n",
    "        class_index = 0\n",
    "        for plant_name in sorted(os.listdir(self.data_dir)):\n",
    "            plant_path = os.path.join(self.data_dir, plant_name)\n",
    "            if os.path.isdir(plant_path):\n",
    "                print(f\"\\nFound plant: {plant_name}\")\n",
    "                for disease_name in sorted(os.listdir(plant_path)):\n",
    "                    disease_path = os.path.join(plant_path, disease_name)\n",
    "                    if os.path.isdir(disease_path):\n",
    "                        # Count images in this category\n",
    "                        image_count = len([f for f in os.listdir(disease_path) \n",
    "                                         if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))])\n",
    "                        print(f\"  - {disease_name}: {image_count} images\")\n",
    "                        \n",
    "                        # Create class name and mapping\n",
    "                        class_name = f\"{plant_name}_{disease_name}\"\n",
    "                        class_names.append(class_name)\n",
    "                        class_mapping[class_index] = (plant_name, disease_name)\n",
    "                        \n",
    "                        if plant_name not in plants:\n",
    "                            plants.append(plant_name)\n",
    "                        if disease_name not in diseases:\n",
    "                            diseases.append(disease_name)\n",
    "                        \n",
    "                        class_index += 1\n",
    "        \n",
    "        self.plant_names = plants\n",
    "        self.disease_names = diseases\n",
    "        self.class_names = class_names\n",
    "        self.class_mapping = class_mapping\n",
    "        \n",
    "        print(f\"\\nDataset Summary:\")\n",
    "        print(f\"Total plants: {len(plants)} - {plants}\")\n",
    "        print(f\"Total diseases: {len(diseases)} - {diseases}\")\n",
    "        print(f\"Total classes: {len(class_names)}\")\n",
    "        \n",
    "        return class_names, class_mapping\n",
    "\n",
    "    def create_flat_structure(self):\n",
    "        \"\"\"Create a flat structure for keras to process with improved cleanup\"\"\"\n",
    "        import uuid\n",
    "        flat_dir = f\"temp_flat_dataset_{uuid.uuid4().hex[:8]}\"\n",
    "        self.temp_dir = flat_dir\n",
    "        \n",
    "        # Clean up any existing temp directories first\n",
    "        self.cleanup_old_temp_dirs()\n",
    "        \n",
    "        if os.path.exists(flat_dir):\n",
    "            print(f\"Removing existing {flat_dir}...\")\n",
    "            self.robust_rmtree(flat_dir)\n",
    "        \n",
    "        os.makedirs(flat_dir)\n",
    "        print(f\"Creating flat dataset structure in {flat_dir}...\")\n",
    "        \n",
    "        try:\n",
    "            for plant_name in os.listdir(self.data_dir):\n",
    "                plant_path = os.path.join(self.data_dir, plant_name)\n",
    "                if os.path.isdir(plant_path):\n",
    "                    for disease_name in os.listdir(plant_path):\n",
    "                        disease_path = os.path.join(plant_path, disease_name)\n",
    "                        if os.path.isdir(disease_path):\n",
    "                            # Create flattened class name\n",
    "                            flat_class_name = f\"{plant_name}_{disease_name}\"\n",
    "                            flat_class_path = os.path.join(flat_dir, flat_class_name)\n",
    "                            \n",
    "                            # Copy the directory\n",
    "                            shutil.copytree(disease_path, flat_class_path)\n",
    "            \n",
    "            print(f\"Flat structure created successfully in {flat_dir}\")\n",
    "            return flat_dir\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating flat structure: {e}\")\n",
    "            # Clean up on error\n",
    "            self.robust_rmtree(flat_dir)\n",
    "            raise\n",
    "\n",
    "    def cleanup_old_temp_dirs(self):\n",
    "        \"\"\"Clean up any old temporary directories\"\"\"\n",
    "        current_dir = os.getcwd()\n",
    "        for item in os.listdir(current_dir):\n",
    "            if item.startswith(\"temp_flat_dataset\") and os.path.isdir(item):\n",
    "                print(f\"Cleaning up old temporary directory: {item}\")\n",
    "                self.robust_rmtree(item)\n",
    "\n",
    "    def load_data(self, validation_split=0.2):\n",
    "        \"\"\"Load and prepare the dataset with memory optimization\"\"\"\n",
    "        # First scan the structure\n",
    "        self.scan_dataset_structure()\n",
    "        \n",
    "        # Create flat structure\n",
    "        flat_data_dir = self.create_flat_structure()\n",
    "        \n",
    "        print(f\"\\nLoading dataset with {len(self.class_names)} classes...\")\n",
    "        print(f\"Using batch size: {self.batch_size}\")\n",
    "        \n",
    "        try:\n",
    "            # Create datasets with memory-efficient settings\n",
    "            self.train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "                flat_data_dir,\n",
    "                validation_split=validation_split,\n",
    "                subset=\"training\",\n",
    "                seed=42,\n",
    "                image_size=self.img_size,\n",
    "                batch_size=self.batch_size,\n",
    "                class_names=self.class_names\n",
    "            )\n",
    "            \n",
    "            self.val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "                flat_data_dir,\n",
    "                validation_split=validation_split,\n",
    "                subset=\"validation\",\n",
    "                seed=42,\n",
    "                image_size=self.img_size,\n",
    "                batch_size=self.batch_size,\n",
    "                class_names=self.class_names\n",
    "            )\n",
    "            \n",
    "            print(f\"Training samples: {len(self.train_ds) * self.batch_size}\")\n",
    "            print(f\"Validation samples: {len(self.val_ds) * self.batch_size}\")\n",
    "            \n",
    "            # Optimize dataset performance\n",
    "            AUTOTUNE = tf.data.AUTOTUNE\n",
    "            self.train_ds = self.train_ds.cache().shuffle(500).prefetch(buffer_size=AUTOTUNE)\n",
    "            self.val_ds = self.val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "            \n",
    "            return self.train_ds, self.val_ds\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            # Clean up on error\n",
    "            if flat_data_dir:\n",
    "                self.robust_rmtree(flat_data_dir)\n",
    "            raise\n",
    "\n",
    "    def create_data_augmentation(self):\n",
    "        \"\"\"Create data augmentation layers\"\"\"\n",
    "        return keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.1),\n",
    "            layers.RandomZoom(0.1),\n",
    "            layers.RandomContrast(0.1),\n",
    "        ])\n",
    "\n",
    "    def create_model(self, use_transfer_learning=True, lightweight=True):\n",
    "        \"\"\"Create the model architecture\"\"\"\n",
    "        num_classes = len(self.class_names)\n",
    "        \n",
    "        if use_transfer_learning:\n",
    "            if lightweight:\n",
    "                base_model = tf.keras.applications.MobileNetV2(\n",
    "                    input_shape=(*self.img_size, 3),\n",
    "                    include_top=False,\n",
    "                    weights='imagenet',\n",
    "                    alpha=0.75\n",
    "                )\n",
    "                base_model.trainable = False\n",
    "                print(f\"Using lightweight MobileNetV2 (alpha=0.75)\")\n",
    "            else:\n",
    "                base_model = tf.keras.applications.MobileNetV2(\n",
    "                    input_shape=(*self.img_size, 3),\n",
    "                    include_top=False,\n",
    "                    weights='imagenet'\n",
    "                )\n",
    "                base_model.trainable = False\n",
    "                print(f\"Using standard MobileNetV2\")\n",
    "            \n",
    "            data_augmentation = self.create_data_augmentation()\n",
    "            preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "            \n",
    "            inputs = keras.Input(shape=(*self.img_size, 3))\n",
    "            x = data_augmentation(inputs)\n",
    "            x = preprocess_input(x)\n",
    "            x = base_model(x, training=False)\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "            outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "            model = keras.Model(inputs, outputs)\n",
    "            \n",
    "        else:\n",
    "            data_augmentation = self.create_data_augmentation()\n",
    "            model = keras.Sequential([\n",
    "                data_augmentation,\n",
    "                layers.Rescaling(1./255),\n",
    "                layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling2D(),\n",
    "                layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling2D(),\n",
    "                layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling2D(),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(256, activation='relu'),\n",
    "                layers.Dropout(0.3),\n",
    "                layers.Dense(num_classes, activation='softmax')\n",
    "            ])\n",
    "            print(\"Using lightweight custom CNN\")\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def compile_model(self, learning_rate=0.001):\n",
    "        \"\"\"Compile the model\"\"\"\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(\"Model compiled successfully!\")\n",
    "        print(f\"Model has {self.model.count_params():,} parameters\")\n",
    "\n",
    "    def train_model(self, epochs=15, callbacks=None):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if callbacks is None:\n",
    "            callbacks = [\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss', \n",
    "                    patience=5, \n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                ),\n",
    "                keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss', \n",
    "                    factor=0.2, \n",
    "                    patience=3, \n",
    "                    min_lr=1e-7,\n",
    "                    verbose=1\n",
    "                )\n",
    "            ]\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        print(f\"Batch size: {self.batch_size}\")\n",
    "        print(f\"Max epochs: {epochs}\")\n",
    "        \n",
    "        try:\n",
    "            self.history = self.model.fit(\n",
    "                self.train_ds,\n",
    "                validation_data=self.val_ds,\n",
    "                epochs=epochs,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Training error: {e}\")\n",
    "            raise\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"No training history available. Train the model first.\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        ax1.plot(self.history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "        ax1.plot(self.history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "        ax1.set_title('Model Accuracy')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot loss\n",
    "        ax2.plot(self.history.history['loss'], label='Training Loss', marker='o')\n",
    "        ax2.plot(self.history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "        ax2.set_title('Model Loss')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Evaluate the model and show detailed metrics\"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        \n",
    "        y_pred, y_true = [], []\n",
    "        for images, labels in self.val_ds:\n",
    "            predictions = self.model.predict(images, verbose=0)\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "            y_true.extend(labels.numpy())\n",
    "        \n",
    "        unique_classes = sorted(list(set(y_true + y_pred)))\n",
    "        present_class_names = [self.class_names[i] for i in unique_classes]\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred, \n",
    "                                  labels=unique_classes,\n",
    "                                  target_names=present_class_names))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=unique_classes)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=[name.replace('_', '\\n') for name in present_class_names], \n",
    "                    yticklabels=[name.replace('_', '\\n') for name in present_class_names])\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        accuracy = np.sum(np.array(y_true) == np.array(y_pred)) / len(y_true)\n",
    "        print(f\"\\nOverall Validation Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "    def save_model(self, filename=\"plant_disease_model.keras\"):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        models_dir = \"../models\"\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "        filepath = os.path.join(models_dir, filename)\n",
    "        \n",
    "        self.model.save(filepath)\n",
    "        \n",
    "        # Save metadata\n",
    "        import pickle\n",
    "        metadata = {\n",
    "            'class_names': self.class_names,\n",
    "            'class_mapping': self.class_mapping,\n",
    "            'plant_names': self.plant_names,\n",
    "            'disease_names': self.disease_names,\n",
    "            'img_size': self.img_size\n",
    "        }\n",
    "        \n",
    "        metadata_filename = filename.replace('.keras', '_metadata.pkl')\n",
    "        metadata_filepath = os.path.join(models_dir, metadata_filename)\n",
    "        \n",
    "        with open(metadata_filepath, 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"Model saved to {filepath}\")\n",
    "        print(f\"Metadata saved to {metadata_filepath}\")\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Enhanced cleanup with robust directory removal\"\"\"\n",
    "        print(\"Starting cleanup...\")\n",
    "        \n",
    "        # Close datasets first\n",
    "        if hasattr(self, 'train_ds') and self.train_ds is not None:\n",
    "            del self.train_ds\n",
    "            self.train_ds = None\n",
    "        \n",
    "        if hasattr(self, 'val_ds') and self.val_ds is not None:\n",
    "            del self.val_ds\n",
    "            self.val_ds = None\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Clean up the specific temp directory if it exists\n",
    "        if hasattr(self, 'temp_dir') and self.temp_dir and os.path.exists(self.temp_dir):\n",
    "            print(f\"Removing temporary directory: {self.temp_dir}\")\n",
    "            success = self.robust_rmtree(self.temp_dir)\n",
    "            if success:\n",
    "                print(\"Cleanup completed successfully!\")\n",
    "            else:\n",
    "                print(\"Cleanup completed with some issues - check for remaining temp directories\")\n",
    "        \n",
    "        # Clean up any other temp directories\n",
    "        self.cleanup_old_temp_dirs()\n",
    "        \n",
    "        print(\"Cleanup process finished.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with improved error handling\"\"\"\n",
    "    data_dir = \"my_diseases_dataset\"\n",
    "    classifier = None\n",
    "    \n",
    "    try:\n",
    "        # Initialize classifier\n",
    "        print(\"Initializing classifier with memory optimization...\")\n",
    "        classifier = PlantDiseaseClassifier(data_dir, img_size=(224, 224), batch_size=8)\n",
    "        \n",
    "        # Load data\n",
    "        print(\"Loading dataset...\")\n",
    "        train_ds, val_ds = classifier.load_data(validation_split=0.2)\n",
    "        \n",
    "        # Create model\n",
    "        print(\"Creating model...\")\n",
    "        model = classifier.create_model(use_transfer_learning=True, lightweight=True)\n",
    "        classifier.compile_model(learning_rate=0.001)\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training model...\")\n",
    "        history = classifier.train_model(epochs=10)\n",
    "        \n",
    "        # Evaluate\n",
    "        accuracy = classifier.evaluate_model()\n",
    "        \n",
    "        # Save model\n",
    "        classifier.save_model(\"plant_disease_model_fixed.keras\")\n",
    "        \n",
    "        print(f\"\\nTraining completed successfully!\")\n",
    "        print(f\"Final validation accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Dataset directory '{data_dir}' not found.\")\n",
    "        print(\"Please ensure your dataset is in the correct location.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        # Always try to cleanup, even if there were errors\n",
    "        if classifier is not None:\n",
    "            try:\n",
    "                classifier.cleanup()\n",
    "            except Exception as cleanup_error:\n",
    "                print(f\"Error during cleanup: {cleanup_error}\")\n",
    "                print(\"You may need to manually delete temporary directories\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465c8e9-eb85-489f-bddf-6ade71223a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
