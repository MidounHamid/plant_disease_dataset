{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3410ba5b-7a6a-4a34-8c37-d6d3fdd75018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning dataset structure...\n",
      "\n",
      "Found plant: apple\n",
      "  - Alternaria leaf spot: 278 images\n",
      "  - Brown spot: 215 images\n",
      "  - Gray spot: 395 images\n",
      "  - Healthy leaf: 409 images\n",
      "  - Rust: 344 images\n",
      "\n",
      "Found plant: tomato\n",
      "  - Tomato___Bacterial_spot: 1000 images\n",
      "  - Tomato___Early_blight: 1000 images\n",
      "  - Tomato___Late_blight: 1000 images\n",
      "  - Tomato___Leaf_Mold: 1000 images\n",
      "  - Tomato___Septoria_leaf_spot: 1000 images\n",
      "  - Tomato___Spider_mites Two-spotted_spider_mite: 1000 images\n",
      "  - Tomato___Target_Spot: 9 images\n",
      "\n",
      "Dataset Summary:\n",
      "Total plants: 2 - ['apple', 'tomato']\n",
      "Total diseases: 12 - ['Alternaria leaf spot', 'Brown spot', 'Gray spot', 'Healthy leaf', 'Rust', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot']\n",
      "Total classes: 12\n",
      "Creating flat dataset structure...\n",
      "\n",
      "Loading dataset with 12 classes...\n",
      "Found 7650 files belonging to 12 classes.\n",
      "Using 6120 files for training.\n",
      "Found 7650 files belonging to 12 classes.\n",
      "Using 1530 files for validation.\n",
      "Training samples: 6144\n",
      "Validation samples: 1536\n",
      "Model compiled successfully!\n",
      "Model has 2,273,356 parameters\n",
      "Starting training...\n",
      "Epoch 1/20\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 1s/step - accuracy: 0.4714 - loss: 1.5419 - val_accuracy: 0.7627 - val_loss: 0.7268 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 1s/step - accuracy: 0.7800 - loss: 0.6601 - val_accuracy: 0.8163 - val_loss: 0.5890 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m154/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.8029 - loss: 0.5626Error: Graph execution error:\n",
      "\n",
      "Detected at node StatefulPartitionedCall/functional_1_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution defined at (most recent call last):\n",
      "<stack traces unavailable>\n",
      "Operation received an exception:Status: 1, message: could not create a memory object, in file tensorflow/core/kernels/mkl/mkl_conv_ops.cc:1112\n",
      "\t [[{{node StatefulPartitionedCall/functional_1_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution}}]] [Op:__inference_multi_step_on_iterator_9102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\D-TECH Services\\AppData\\Local\\Temp\\ipykernel_20336\\2996966869.py\", line 563, in main\n",
      "    history = classifier.train_model(epochs=20)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\D-TECH Services\\AppData\\Local\\Temp\\ipykernel_20336\\2996966869.py\", line 234, in train_model\n",
      "    self.history = self.model.fit(\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\D-TECH Services\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\D-TECH Services\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\n",
      "    except TypeError as e:\n",
      "tensorflow.python.framework.errors_impl.AbortedError: Graph execution error:\n",
      "\n",
      "Detected at node StatefulPartitionedCall/functional_1_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution defined at (most recent call last):\n",
      "<stack traces unavailable>\n",
      "Operation received an exception:Status: 1, message: could not create a memory object, in file tensorflow/core/kernels/mkl/mkl_conv_ops.cc:1112\n",
      "\t [[{{node StatefulPartitionedCall/functional_1_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution}}]] [Op:__inference_multi_step_on_iterator_9102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary files cleaned up.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class PlantDiseaseClassifier:\n",
    "    def __init__(self, data_dir, img_size=(224, 224), batch_size=32):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.class_names = None\n",
    "        self.plant_names = None\n",
    "        self.disease_names = None\n",
    "        self.class_mapping = {}  # Maps class_index to (plant, disease)\n",
    "\n",
    "    def scan_dataset_structure(self):\n",
    "        \"\"\"Scan the dataset to understand the structure and create class mappings\"\"\"\n",
    "        print(\"Scanning dataset structure...\")\n",
    "        \n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise FileNotFoundError(f\"Dataset directory '{self.data_dir}' not found.\")\n",
    "        \n",
    "        plants = []\n",
    "        diseases = []\n",
    "        class_names = []\n",
    "        class_mapping = {}\n",
    "        \n",
    "        class_index = 0\n",
    "        for plant_name in sorted(os.listdir(self.data_dir)):\n",
    "            plant_path = os.path.join(self.data_dir, plant_name)\n",
    "            if os.path.isdir(plant_path):\n",
    "                print(f\"\\nFound plant: {plant_name}\")\n",
    "                for disease_name in sorted(os.listdir(plant_path)):\n",
    "                    disease_path = os.path.join(plant_path, disease_name)\n",
    "                    if os.path.isdir(disease_path):\n",
    "                        # Count images in this category\n",
    "                        image_count = len([f for f in os.listdir(disease_path) \n",
    "                                         if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))])\n",
    "                        print(f\"  - {disease_name}: {image_count} images\")\n",
    "                        \n",
    "                        # Create class name and mapping\n",
    "                        class_name = f\"{plant_name}_{disease_name}\"\n",
    "                        class_names.append(class_name)\n",
    "                        class_mapping[class_index] = (plant_name, disease_name)\n",
    "                        \n",
    "                        if plant_name not in plants:\n",
    "                            plants.append(plant_name)\n",
    "                        if disease_name not in diseases:\n",
    "                            diseases.append(disease_name)\n",
    "                        \n",
    "                        class_index += 1\n",
    "        \n",
    "        self.plant_names = plants\n",
    "        self.disease_names = diseases\n",
    "        self.class_names = class_names\n",
    "        self.class_mapping = class_mapping\n",
    "        \n",
    "        print(f\"\\nDataset Summary:\")\n",
    "        print(f\"Total plants: {len(plants)} - {plants}\")\n",
    "        print(f\"Total diseases: {len(diseases)} - {diseases}\")\n",
    "        print(f\"Total classes: {len(class_names)}\")\n",
    "        \n",
    "        return class_names, class_mapping\n",
    "\n",
    "    def create_flat_structure(self):\n",
    "        \"\"\"Create a flat structure for keras to process\"\"\"\n",
    "        flat_dir = \"temp_flat_dataset\"\n",
    "        if os.path.exists(flat_dir):\n",
    "            shutil.rmtree(flat_dir)\n",
    "        os.makedirs(flat_dir)\n",
    "        \n",
    "        print(\"Creating flat dataset structure...\")\n",
    "        \n",
    "        for plant_name in os.listdir(self.data_dir):\n",
    "            plant_path = os.path.join(self.data_dir, plant_name)\n",
    "            if os.path.isdir(plant_path):\n",
    "                for disease_name in os.listdir(plant_path):\n",
    "                    disease_path = os.path.join(plant_path, disease_name)\n",
    "                    if os.path.isdir(disease_path):\n",
    "                        # Create flattened class name\n",
    "                        flat_class_name = f\"{plant_name}_{disease_name}\"\n",
    "                        flat_class_path = os.path.join(flat_dir, flat_class_name)\n",
    "                        \n",
    "                        # Copy the directory\n",
    "                        shutil.copytree(disease_path, flat_class_path)\n",
    "        \n",
    "        return flat_dir\n",
    "\n",
    "    def load_data(self, validation_split=0.2):\n",
    "        \"\"\"Load and prepare the dataset\"\"\"\n",
    "        # First scan the structure\n",
    "        self.scan_dataset_structure()\n",
    "        \n",
    "        # Create flat structure\n",
    "        flat_data_dir = self.create_flat_structure()\n",
    "        \n",
    "        print(f\"\\nLoading dataset with {len(self.class_names)} classes...\")\n",
    "        \n",
    "        # Create datasets\n",
    "        self.train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            flat_data_dir,\n",
    "            validation_split=validation_split,\n",
    "            subset=\"training\",\n",
    "            seed=42,\n",
    "            image_size=self.img_size,\n",
    "            batch_size=self.batch_size,\n",
    "            class_names=self.class_names  # Ensure consistent class ordering\n",
    "        )\n",
    "        \n",
    "        self.val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            flat_data_dir,\n",
    "            validation_split=validation_split,\n",
    "            subset=\"validation\",\n",
    "            seed=42,\n",
    "            image_size=self.img_size,\n",
    "            batch_size=self.batch_size,\n",
    "            class_names=self.class_names  # Ensure consistent class ordering\n",
    "        )\n",
    "        \n",
    "        print(f\"Training samples: {len(self.train_ds) * self.batch_size}\")\n",
    "        print(f\"Validation samples: {len(self.val_ds) * self.batch_size}\")\n",
    "        \n",
    "        # Optimize dataset performance\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "        self.train_ds = self.train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "        self.val_ds = self.val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "        \n",
    "        return self.train_ds, self.val_ds\n",
    "\n",
    "    def create_data_augmentation(self):\n",
    "        \"\"\"Create data augmentation layers\"\"\"\n",
    "        return keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "            layers.RandomRotation(0.2),\n",
    "            layers.RandomZoom(0.2),\n",
    "            layers.RandomContrast(0.2),\n",
    "            layers.RandomBrightness(0.2),\n",
    "        ])\n",
    "\n",
    "    def create_model(self, use_transfer_learning=True):\n",
    "        \"\"\"Create the model architecture\"\"\"\n",
    "        num_classes = len(self.class_names)\n",
    "        \n",
    "        if use_transfer_learning:\n",
    "            # Transfer learning with MobileNetV2\n",
    "            base_model = tf.keras.applications.MobileNetV2(\n",
    "                input_shape=(*self.img_size, 3),\n",
    "                include_top=False,\n",
    "                weights='imagenet'\n",
    "            )\n",
    "            base_model.trainable = False\n",
    "            \n",
    "            data_augmentation = self.create_data_augmentation()\n",
    "            preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "            \n",
    "            inputs = keras.Input(shape=(*self.img_size, 3))\n",
    "            x = data_augmentation(inputs)\n",
    "            x = preprocess_input(x)\n",
    "            x = base_model(x, training=False)\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "            outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "            model = keras.Model(inputs, outputs)\n",
    "        else:\n",
    "            # Custom CNN architecture\n",
    "            data_augmentation = self.create_data_augmentation()\n",
    "            model = keras.Sequential([\n",
    "                data_augmentation,\n",
    "                layers.Rescaling(1./255),\n",
    "                layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling2D(),\n",
    "                layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling2D(),\n",
    "                layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling2D(),\n",
    "                layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.MaxPooling2D(),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(512, activation='relu'),\n",
    "                layers.Dropout(0.5),\n",
    "                layers.Dense(256, activation='relu'),\n",
    "                layers.Dropout(0.3),\n",
    "                layers.Dense(num_classes, activation='softmax')\n",
    "            ])\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def compile_model(self, learning_rate=0.001):\n",
    "        \"\"\"Compile the model\"\"\"\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        print(\"Model compiled successfully!\")\n",
    "        print(f\"Model has {self.model.count_params():,} parameters\")\n",
    "\n",
    "    def train_model(self, epochs=20, callbacks=None):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if callbacks is None:\n",
    "            callbacks = [\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss', \n",
    "                    patience=5, \n",
    "                    restore_best_weights=True,\n",
    "                    verbose=1\n",
    "                ),\n",
    "                keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss', \n",
    "                    factor=0.2, \n",
    "                    patience=3, \n",
    "                    min_lr=1e-7,\n",
    "                    verbose=1\n",
    "                )\n",
    "            ]\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        self.history = self.model.fit(\n",
    "            self.train_ds,\n",
    "            validation_data=self.val_ds,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        return self.history\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"No training history available. Train the model first.\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        ax1.plot(self.history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "        ax1.plot(self.history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "        ax1.set_title('Model Accuracy')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot loss\n",
    "        ax2.plot(self.history.history['loss'], label='Training Loss', marker='o')\n",
    "        ax2.plot(self.history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "        ax2.set_title('Model Loss')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Evaluate the model and show detailed metrics\"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        \n",
    "        y_pred, y_true = [], []\n",
    "        for images, labels in self.val_ds:\n",
    "            predictions = self.model.predict(images, verbose=0)\n",
    "            y_pred.extend(np.argmax(predictions, axis=1))\n",
    "            y_true.extend(labels.numpy())\n",
    "        \n",
    "        # Get unique classes present in the validation set\n",
    "        unique_classes = sorted(list(set(y_true + y_pred)))\n",
    "        present_class_names = [self.class_names[i] for i in unique_classes]\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred, \n",
    "                                  labels=unique_classes,\n",
    "                                  target_names=present_class_names))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=unique_classes)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=present_class_names, \n",
    "                    yticklabels=present_class_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        accuracy = np.sum(np.array(y_true) == np.array(y_pred)) / len(y_true)\n",
    "        print(f\"\\nOverall Validation Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Show per-plant and per-disease accuracy\n",
    "        self.show_detailed_results(y_true, y_pred, unique_classes)\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "    def show_detailed_results(self, y_true, y_pred, unique_classes):\n",
    "        \"\"\"Show detailed results by plant and disease\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DETAILED RESULTS BY PLANT AND DISEASE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Group results by plant\n",
    "        plant_results = {}\n",
    "        disease_results = {}\n",
    "        \n",
    "        for i, (true_idx, pred_idx) in enumerate(zip(y_true, y_pred)):\n",
    "            if true_idx in unique_classes and pred_idx in unique_classes:\n",
    "                true_plant, true_disease = self.class_mapping[true_idx]\n",
    "                pred_plant, pred_disease = self.class_mapping[pred_idx]\n",
    "                \n",
    "                # Plant-level accuracy\n",
    "                if true_plant not in plant_results:\n",
    "                    plant_results[true_plant] = {'correct': 0, 'total': 0}\n",
    "                plant_results[true_plant]['total'] += 1\n",
    "                if true_plant == pred_plant:\n",
    "                    plant_results[true_plant]['correct'] += 1\n",
    "                \n",
    "                # Disease-level accuracy\n",
    "                if true_disease not in disease_results:\n",
    "                    disease_results[true_disease] = {'correct': 0, 'total': 0}\n",
    "                disease_results[true_disease]['total'] += 1\n",
    "                if true_disease == pred_disease:\n",
    "                    disease_results[true_disease]['correct'] += 1\n",
    "        \n",
    "        # Display plant-level results\n",
    "        print(\"\\nPLANT IDENTIFICATION ACCURACY:\")\n",
    "        print(\"-\" * 40)\n",
    "        for plant, results in plant_results.items():\n",
    "            accuracy = results['correct'] / results['total']\n",
    "            print(f\"{plant:15}: {accuracy:.3f} ({results['correct']}/{results['total']})\")\n",
    "        \n",
    "        # Display disease-level results\n",
    "        print(\"\\nDISEASE IDENTIFICATION ACCURACY:\")\n",
    "        print(\"-\" * 40)\n",
    "        for disease, results in disease_results.items():\n",
    "            accuracy = results['correct'] / results['total']\n",
    "            print(f\"{disease:20}: {accuracy:.3f} ({results['correct']}/{results['total']})\")\n",
    "\n",
    "    def predict_image(self, image_path, show_image=True):\n",
    "        \"\"\"Predict disease for a single image\"\"\"\n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFoundError(f\"Image file '{image_path}' not found.\")\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img = keras.utils.load_img(image_path, target_size=self.img_size)\n",
    "        img_array = keras.utils.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array, 0)\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = self.model.predict(img_array, verbose=0)\n",
    "        predicted_class_idx = np.argmax(predictions[0])\n",
    "        confidence = float(np.max(predictions[0]))\n",
    "        \n",
    "        # Get plant and disease names\n",
    "        plant_name, disease_name = self.class_mapping[predicted_class_idx]\n",
    "        class_name = self.class_names[predicted_class_idx]\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PLANT DISEASE DIAGNOSIS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Plant Type:     {plant_name.upper()}\")\n",
    "        print(f\"Disease:        {disease_name.upper()}\")\n",
    "        print(f\"Confidence:     {confidence:.2%}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Show image if requested\n",
    "        if show_image:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Plant: {plant_name} | Disease: {disease_name}\\nConfidence: {confidence:.2%}\")\n",
    "            plt.show()\n",
    "        \n",
    "        return plant_name, disease_name, confidence\n",
    "\n",
    "    def save_model(self, filename=\"plant_disease_model.keras\"):\n",
    "        \"\"\"Save the trained model and metadata to ../models directory\"\"\"\n",
    "        # Create the models directory\n",
    "        models_dir = \"../models\"\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "        \n",
    "        # Full filepath\n",
    "        filepath = os.path.join(models_dir, filename)\n",
    "        \n",
    "        # Save the model\n",
    "        self.model.save(filepath)\n",
    "        \n",
    "        # Save class mappings and metadata\n",
    "        import pickle\n",
    "        metadata = {\n",
    "            'class_names': self.class_names,\n",
    "            'class_mapping': self.class_mapping,\n",
    "            'plant_names': self.plant_names,\n",
    "            'disease_names': self.disease_names,\n",
    "            'img_size': self.img_size\n",
    "        }\n",
    "        \n",
    "        metadata_filename = filename.replace('.keras', '_metadata.pkl')\n",
    "        metadata_filepath = os.path.join(models_dir, metadata_filename)\n",
    "        \n",
    "        with open(metadata_filepath, 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(f\"Model saved to {filepath}\")\n",
    "        print(f\"Metadata saved to {metadata_filepath}\")\n",
    "        \n",
    "        # List all files in the models directory\n",
    "        print(f\"\\nFiles in {models_dir}:\")\n",
    "        files = os.listdir(models_dir)\n",
    "        for file in sorted(files):\n",
    "            file_path = os.path.join(models_dir, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "                print(f\"  - {file} ({size:.2f} MB)\")\n",
    "\n",
    "    def load_model(self, filename=\"plant_disease_model.keras\"):\n",
    "        \"\"\"Load a saved model and metadata from ../models directory\"\"\"\n",
    "        models_dir = \"../models\"\n",
    "        filepath = os.path.join(models_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Model file '{filepath}' not found.\")\n",
    "        \n",
    "        # Load the model\n",
    "        self.model = keras.models.load_model(filepath)\n",
    "        \n",
    "        # Load class mappings and metadata\n",
    "        import pickle\n",
    "        metadata_filename = filename.replace('.keras', '_metadata.pkl')\n",
    "        metadata_filepath = os.path.join(models_dir, metadata_filename)\n",
    "        \n",
    "        if os.path.exists(metadata_filepath):\n",
    "            with open(metadata_filepath, 'rb') as f:\n",
    "                metadata = pickle.load(f)\n",
    "            \n",
    "            self.class_names = metadata['class_names']\n",
    "            self.class_mapping = metadata['class_mapping']\n",
    "            self.plant_names = metadata['plant_names']\n",
    "            self.disease_names = metadata['disease_names']\n",
    "            self.img_size = metadata['img_size']\n",
    "            \n",
    "            print(f\"Model and metadata loaded from {filepath}\")\n",
    "            print(f\"Model supports {len(self.class_names)} classes:\")\n",
    "            for i, class_name in enumerate(self.class_names):\n",
    "                plant, disease = self.class_mapping[i]\n",
    "                print(f\"  {i}: {plant} - {disease}\")\n",
    "        else:\n",
    "            print(f\"Model loaded from {filepath}, but metadata file not found\")\n",
    "            print(\"You may need to retrain or manually set class mappings\")\n",
    "\n",
    "    def list_saved_models(self):\n",
    "        \"\"\"List all saved models in the ../models directory\"\"\"\n",
    "        models_dir = \"../models\"\n",
    "        if not os.path.exists(models_dir):\n",
    "            print(f\"Models directory '{models_dir}' does not exist.\")\n",
    "            return []\n",
    "        \n",
    "        model_files = []\n",
    "        files = os.listdir(models_dir)\n",
    "        \n",
    "        print(f\"\\nSaved models in {models_dir}:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for file in sorted(files):\n",
    "            if file.endswith('.keras'):\n",
    "                file_path = os.path.join(models_dir, file)\n",
    "                size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "                \n",
    "                # Check for corresponding metadata file\n",
    "                metadata_file = file.replace('.keras', '_metadata.pkl')\n",
    "                metadata_path = os.path.join(models_dir, metadata_file)\n",
    "                has_metadata = os.path.exists(metadata_path)\n",
    "                \n",
    "                model_files.append(file)\n",
    "                status = \"✓\" if has_metadata else \"✗\"\n",
    "                print(f\"  {status} {file} ({size:.2f} MB)\")\n",
    "                if has_metadata:\n",
    "                    metadata_size = os.path.getsize(metadata_path) / 1024  # Size in KB\n",
    "                    print(f\"    └── {metadata_file} ({metadata_size:.2f} KB)\")\n",
    "        \n",
    "        if not model_files:\n",
    "            print(\"  No saved models found.\")\n",
    "        \n",
    "        return model_files\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up temporary files\"\"\"\n",
    "        if os.path.exists(\"temp_flat_dataset\"):\n",
    "            shutil.rmtree(\"temp_flat_dataset\")\n",
    "            print(\"Temporary files cleaned up.\")\n",
    "\n",
    "def create_sample_dataset():\n",
    "    \"\"\"Create a sample dataset structure for testing\"\"\"\n",
    "    print(\"Creating sample dataset structure...\")\n",
    "    dataset_dir = \"my_diseases_dataset\"\n",
    "    \n",
    "    # Create nested directory structure - Only Apple and Tomato\n",
    "    plant_diseases = {\n",
    "        \"apple\": [\"healthy\", \"scab\", \"rust\", \"powdery_mildew\"],\n",
    "        \"tomato\": [\"healthy\", \"early_blight\", \"late_blight\", \"bacterial_spot\"]\n",
    "    }\n",
    "    \n",
    "    for plant, diseases in plant_diseases.items():\n",
    "        for disease in diseases:\n",
    "            path = os.path.join(dataset_dir, plant, disease)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Sample dataset structure created at: {dataset_dir}\")\n",
    "    print(\"\\nDirectory structure:\")\n",
    "    for plant, diseases in plant_diseases.items():\n",
    "        print(f\"{plant}/\")\n",
    "        for disease in diseases:\n",
    "            print(f\"  ├── {disease}/\")\n",
    "    \n",
    "    print(\"\\nPlease add your plant disease images to the respective folders.\")\n",
    "    return dataset_dir\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate usage\"\"\"\n",
    "    data_dir = \"my_diseases_dataset\"\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Dataset directory '{data_dir}' not found.\")\n",
    "        print(\"\\nOptions:\")\n",
    "        print(\"1. Create sample structure with create_sample_dataset()\")\n",
    "        print(\"2. Or put your existing dataset in the 'my_diseases_dataset' folder\")\n",
    "        print(\"3. Or change 'data_dir' variable to point to your dataset location\")\n",
    "        create_sample_dataset()\n",
    "        print(\"Please add your images to the dataset folders and run again.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize classifier\n",
    "    classifier = PlantDiseaseClassifier(data_dir, img_size=(224, 224), batch_size=32)\n",
    "    \n",
    "    try:\n",
    "        # Load and scan data\n",
    "        train_ds, val_ds = classifier.load_data(validation_split=0.2)\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = classifier.create_model(use_transfer_learning=True)\n",
    "        classifier.compile_model(learning_rate=0.001)\n",
    "        \n",
    "        # Train model\n",
    "        history = classifier.train_model(epochs=20)\n",
    "        \n",
    "        # Plot training history\n",
    "        classifier.plot_training_history()\n",
    "        \n",
    "        # Evaluate model\n",
    "        accuracy = classifier.evaluate_model()\n",
    "        \n",
    "        # Save model to ../models directory\n",
    "        classifier.save_model(\"plant_disease_model.keras\")\n",
    "        \n",
    "        # List all saved models\n",
    "        classifier.list_saved_models()\n",
    "        \n",
    "        print(f\"\\nTraining completed successfully!\")\n",
    "        print(f\"Final validation accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Example of loading the model\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Testing model loading...\")\n",
    "        new_classifier = PlantDiseaseClassifier(data_dir)\n",
    "        new_classifier.load_model(\"plant_disease_model.keras\")\n",
    "        \n",
    "        # Example prediction (uncomment and provide image path)\n",
    "        # classifier.predict_image(\"path/to/your/test_image.jpg\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        classifier.cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4001f935-40bb-429d-9581-458994b65c97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
